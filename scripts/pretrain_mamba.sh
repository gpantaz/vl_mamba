deepspeed src/vl_mamba/train_hf.py \
	--model_name state-spaces/mamba-2.8b \
	--train_only_visual_embeddings True \
	--image_size 224 \
	--patch_size 32 \
	--num_channels 3 \
	--variable_sized False \
	--box_mode normalize \
	--tokenizer_name EleutherAI/gpt-neox-20b \
	--tokenizer_truncation_side right \
	--tokenizer_padding_side right \
	--tokenizer_add_special_tokens True \
	--model_max_length 128 \
	--dataset_path src/vl_mamba/datasets/vl_mamba \
	--dataset_cache_dir /mnt/ceph_rbd/storage/datasets/vl_mamba \
	--root_dataset_path /mnt/ceph_rbd/storage/datasets/ \
	--train_dataset_subset llava_pretrain \
	--eval_dataset_subset llava_pretrain \
	--output_dir storage/checkpoints/mamba-2.8b_imgsz224_psz32 \
	--per_device_train_batch_size 64 \
	--per_device_eval_batch_size 4 \
	--gradient_accumulation_steps 1 \
	--logging_steps 1 \
	--save_strategy "steps" \
	--save_steps 0.2 \
	--num_train_epochs 1 \
	--learning_rate 1e-3 \
	--weight_decay 0. \
	--warmup_ratio 0.03 \
	--lr_scheduler_type cosine \
	--bf16 True \
	--fp16 False \
	--gradient_checkpointing False \
	--deepspeed configs/trainer/zero2.json \
	--save_total_limit 1 \
	--load_best_model_at_end False \
	--log_level info \
	--save_safetensors True \
	--evaluation_strategy no \
	--eval_steps 0.1 \
	--seed 12345 \
	--data_seed 12345 \
	--dataloader_num_workers 4 \
	--logging_nan_inf_filter False \
	--run_name pretrain-mamba-2.8b_imgsz224_psz32 \
	--project_name vl-mamba \
	--report_to wandb \
